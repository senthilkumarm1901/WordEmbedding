{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a folder path as an argument with '--log_dir' to save\n",
    "# TensorBoard summaries. Default is a log folder in current directory.\n",
    "current_path = os.path.dirname(os.path.realpath(path_to_give))\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--log_dir',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'log'),\n",
    "    help='The log directory for TensorBoard summaries.')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# Create the directory for TensorBoard variables if there is not.\n",
    "if not os.path.exists(FLAGS.log_dir):\n",
    "    os.makedirs(FLAGS.log_dir)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Text8 is generated from Wikipedia: \n",
    "It is a clean version of the Wikipedia was prepared with the goal of retaining only text that normally would be visible when displayed on a Wikipedia web page and read by a human. Only regular article text was retained. Image captions were retained, but tables and links to foreign language versions were removed. Citations, footnotes, and markup were removed. Hypertext links were converted to ordinary text, retaining only the (visible) anchor text. Numbers were spelled out (\"20\" becomes \"two zero\", a common practice in speech research). Upper case letters were converted to lower case. Finally, all sequences of characters not in the range a-z were converted to a single space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps are from Tensorflow's example word2vec implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful Links: <br>\n",
    "    1. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py \n",
    "    2. https://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/\n",
    "    3. https://github.com/adventuresinML/adventures-in-ml-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1: Read the data into a list of strings __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = read_data('text8.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism', 'originated', 'as', 'a', 'term']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)\n",
    "#total no. of words in the vocabulary (not unique list of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253854"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(vocabulary))\n",
    "#Vocabulary Size = no. of unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2: Build the dictionary and replace rare words with UNK token.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 20000 #setting the upper limit to 20,000 \n",
    "#the below fn does the following:  \n",
    "##any words not within the top 20,000 most common words will be marked with an “UNK” designation, standing for “unknown”\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "\t\"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "\tcount = [['UNK', -1]]\n",
    "\t#count the number of words in the given argument (words) and then returns the 'n_words' (which is 20,000) most common words in a list format\n",
    "\tcount.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "\tdictionary = dict()\n",
    "\tfor word, _ in count:\n",
    "\t\tdictionary[word] = len(dictionary)\n",
    "\tdata = list()\n",
    "\tunk_count = 0\n",
    "\tfor word in words:\n",
    "\t\tindex = dictionary.get(word, 0)\n",
    "\t\tif index == 0:  # dictionary['UNK']\n",
    "\t\t\tunk_count += 1\n",
    "\t\tdata.append(index)\n",
    "\tcount[0][1] = unk_count\n",
    "\treversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\treturn data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 996665], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary, vocabulary_size)\n",
    "del vocabulary  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5]) #common words including UNK token\n",
    "print \n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3: Function to generate a training batch for the skip-gram model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words: <br>\n",
    "To create a data set comprising of our input words and associated grams, which can be used to train our Word2Vec embedding system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__num_skips__ =How many times to reuse an input to generate a label <br>\n",
    "__num_skips__ restrict the number of context words we would use as output words <br>\n",
    "the number of words drawn randomly from the surrounding context is defined by the argument __num_skips__ <br>\n",
    "__skip_window__ = How many words to consider left and right <br>\n",
    "__batch_size__ = the size of the word list that the input word and context samples will be drawn from <br>\n",
    "__buffer__ will hold a maximum of span elements and will be a kind of moving window of words that samples are drawn from <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to generate mini-batches during our training\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "\tglobal data_index\n",
    "\tassert batch_size % num_skips == 0\n",
    "\tassert num_skips <= 2 * skip_window ##if you give skip_window =1, then num_skips should be at max 2. \n",
    "\tbatch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "\tlabels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\tspan = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "\tbuffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builtin\n",
    "\tif data_index + span > len(data):\n",
    "\t\tdata_index = 0\n",
    "\tbuffer.extend(data[data_index:data_index + span])\n",
    "\tdata_index += span\n",
    "\tfor i in range(batch_size // num_skips):\n",
    "\t\tcontext_words = [w for w in range(span) if w != skip_window]\n",
    "\t\twords_to_use = random.sample(context_words, num_skips)\n",
    "\t\tfor j, context_word in enumerate(words_to_use):\n",
    "\t\t\tbatch[i * num_skips + j] = buffer[skip_window] #this is the input word\n",
    "\t\t\tlabels[i * num_skips + j, 0] = buffer[context_word] #these are the context words\n",
    "\t\tif data_index == len(data):\n",
    "\t\t\tbuffer.extend(data[0:span])\n",
    "\t\t\tdata_index = span\n",
    "\t\telse:\n",
    "\t\t\tbuffer.append(data[data_index])\n",
    "\t\t\tdata_index += 1\n",
    "\t# Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "\tdata_index = (data_index + len(data) - span) % len(data)\n",
    "\treturn batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4: Build a skip-gram model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 300  # Dimension of the embedding vector.\n",
    "skip_window = 2  # How many words to consider left and right.\n",
    "num_skips = 4  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64  # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The code __below__ randomly chooses 16 integers from 0-100 – this corresponds to the integer indexes of the most common 100 words in our text data <br>\n",
    "- These will be the words we examine to assess how our learning is progressing in associating related words together in the vector-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Flow (similar to Andrew Ng's course): \n",
    "1. Do Forward Propagation (compute softmax)\n",
    "2. Compute Cost (nce_loss)\n",
    "3. Do Backward Propagation (here gradient descent optimizer)\n",
    "4. Update Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed Steps we do in below code: \n",
    "1. Setup __TensorFlow placeholders__ that will hold our the integer indexes of our input words and \n",
    "    context words which we are trying to predict <br>\n",
    "2. TF Embedding Lookup fn: tf.nn.embedding_lookup(__embeddings__, __train_inputs__) <br>\n",
    "    tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None, validate_indices=True, max_norm=None) <br> The function retrieves __ids__ rows of the __params__ tensor <br> The __partitioning strategy__ is useful for larger scale problems when the matrix might be too large to keep in one piece\n",
    "3. Construct the variables for the softmax (here noise contrastive estimation softmax)\n",
    "4. Construct the loss function (computed via nce_loss function in tensorflow) and keep track of the loss function\n",
    "5. Construct the SGD optimizer using a learning rate of 1.0\n",
    "6. Construct the cosine similarity between minibatch examples and all embeddings and performing validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we only build the graph below, computing the graph is later\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "            \n",
    "    # Input data.\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        with tf.name_scope('embeddings'):\n",
    "            embeddings = tf.Variable(\n",
    "                    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        # Construct the variables for the NCE loss\n",
    "        with tf.name_scope('weights'):\n",
    "            nce_weights = tf.Variable(\n",
    "                    tf.truncated_normal(\n",
    "                            [vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    #     http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(\n",
    "                tf.nn.nce_loss(\n",
    "                        weights=nce_weights,\n",
    "                        biases=nce_biases,\n",
    "                        labels=train_labels,\n",
    "                        inputs=embed,\n",
    "                        num_sampled=num_sampled,\n",
    "                        num_classes=vocabulary_size))\n",
    "\n",
    "    # Add the loss value as a scalar to summary.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Merge all summaries.\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 5: Begin training__ the skip gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How to open tensorboard summaries: \n",
    "    a. go to log_dir where you have saved the summary and type: tensorboard --log_dir=summaries\n",
    "    b. then go to browser and type:  'localhost:6006' , it will open projector in \"http://localhost:6006/#projector\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  254.97625732421875\n",
      "Nearest to as: constantine, harassment, istanbul, chlorophyll, eris, cartoons, ram, cheek,\n",
      "Nearest to than: cohen, features, stoker, infancy, jerry, linguistics, reverend, seldom,\n",
      "Nearest to they: zohar, wessex, deity, calls, sparrow, inter, especially, jose,\n",
      "Nearest to but: plausible, preparing, egoism, wise, conrad, tenn, mmorpgs, charismatic,\n",
      "Nearest to some: consisting, codes, flamenco, fueled, iss, bulgarian, honda, include,\n",
      "Nearest to was: pearls, tariffs, thebes, angles, sacrificial, aleph, sensory, breeding,\n",
      "Nearest to up: deism, hittites, drag, resort, illustrated, yielding, suit, bunny,\n",
      "Nearest to over: quotas, counterpart, muppet, result, sketch, interpreters, ampere, snorri,\n",
      "Nearest to years: weekend, aquaculture, lizard, enduring, king, guido, eduardo, embryo,\n",
      "Nearest to four: roses, lex, corpse, companion, lasted, dynamically, turning, lawsuits,\n",
      "Nearest to states: ceiling, evangelicals, parish, topography, chrono, woodstock, dig, netherlands,\n",
      "Nearest to nine: weakest, slippery, rebuild, honor, overshadowed, setup, measures, equilibrium,\n",
      "Nearest to during: maynard, philippe, fellows, rhyme, ftp, assess, onward, treated,\n",
      "Nearest to other: mclaren, command, ecumenical, unitas, fluoride, academic, visual, anxious,\n",
      "Nearest to be: brothers, consortium, eight, compression, noticeable, friedman, amin, arise,\n",
      "Nearest to no: christmas, trucks, hb, centrist, donna, fond, warhol, having,\n",
      "Average loss at step  2000 :  69.85087936377525\n",
      "Average loss at step  4000 :  21.490967627167702\n",
      "Average loss at step  6000 :  12.111024030208588\n",
      "Average loss at step  8000 :  8.925154497921467\n",
      "Average loss at step  10000 :  7.264266053557396\n",
      "Nearest to as: UNK, and, in, gland, reformation, of, agave, newsgroup,\n",
      "Nearest to than: zero, cohen, gland, and, features, nine, seldom, linguistics,\n",
      "Nearest to they: deity, to, the, deductive, especially, dialects, molecules, calls,\n",
      "Nearest to but: gland, and, egoism, UNK, capture, of, wise, to,\n",
      "Nearest to some: emirates, and, a, operator, include, consisting, sound, codes,\n",
      "Nearest to was: UNK, is, and, one, rockets, a, agave, the,\n",
      "Nearest to up: helmet, frankenstein, ba, resort, neil, illustrated, genuine, gland,\n",
      "Nearest to over: zero, result, newsgroup, ampere, ba, heat, counterpart, quotas,\n",
      "Nearest to years: emirates, agave, debts, rockets, aquaculture, in, ain, UNK,\n",
      "Nearest to four: zero, one, nine, and, gland, reformation, agave, in,\n",
      "Nearest to states: and, evangelicals, netherlands, action, semantics, animated, of, molecules,\n",
      "Nearest to nine: zero, one, gland, two, agave, in, and, the,\n",
      "Nearest to during: in, debts, fellows, anselm, gland, and, recovery, composed,\n",
      "Nearest to other: gland, UNK, compositions, one, command, ncaa, fascism, in,\n",
      "Nearest to be: the, is, agave, canadian, UNK, zero, netherlands, explanation,\n",
      "Nearest to no: molecules, UNK, gland, the, intent, ba, trucks, gauge,\n",
      "Average loss at step  12000 :  7.213326820492744\n",
      "Average loss at step  14000 :  6.385832227945328\n",
      "Average loss at step  16000 :  6.171535402655602\n",
      "Average loss at step  18000 :  5.983292985200882\n",
      "Average loss at step  20000 :  5.664843428373337\n",
      "Nearest to as: and, UNK, is, in, gland, coke, reformation, by,\n",
      "Nearest to than: and, zero, cohen, coke, gland, gb, as, seldom,\n",
      "Nearest to they: to, deity, the, wessex, rotate, he, that, a,\n",
      "Nearest to but: coke, is, and, gland, in, verse, zero, wise,\n",
      "Nearest to some: emirates, a, the, and, zero, is, fueled, stock,\n",
      "Nearest to was: is, and, coke, UNK, are, one, rotate, rockets,\n",
      "Nearest to up: coke, deism, ping, helmet, two, resort, hittites, frankenstein,\n",
      "Nearest to over: zero, coke, five, quotas, newsgroup, alzheimer, result, counterpart,\n",
      "Nearest to years: emirates, weekend, three, coke, agave, zero, debts, aquaculture,\n",
      "Nearest to four: zero, two, nine, one, three, eight, six, coke,\n",
      "Nearest to states: gb, evangelicals, netherlands, ceiling, coke, action, animated, semantics,\n",
      "Nearest to nine: zero, one, eight, six, two, three, four, coke,\n",
      "Nearest to during: in, philippe, gland, debts, and, five, anselm, rudolph,\n",
      "Nearest to other: gland, compositions, guitar, UNK, zero, ncaa, and, acids,\n",
      "Nearest to be: is, mathbf, a, the, was, zero, eight, agave,\n",
      "Nearest to no: a, molecules, intent, gland, rudolph, the, mathbf, ans,\n",
      "Average loss at step  22000 :  5.6663325394392015\n",
      "Average loss at step  24000 :  5.35860156595707\n",
      "Average loss at step  26000 :  5.357403823971748\n",
      "Average loss at step  28000 :  5.266199677824974\n",
      "Average loss at step  30000 :  5.157314746260643\n",
      "Nearest to as: operatorname, UNK, in, coke, gland, is, by, for,\n",
      "Nearest to than: and, zero, operatorname, cohen, coke, gland, as, gb,\n",
      "Nearest to they: to, operatorname, he, circ, deity, acacia, that, it,\n",
      "Nearest to but: operatorname, and, coke, gland, eight, in, circ, is,\n",
      "Nearest to some: the, emirates, operatorname, that, UNK, a, agincourt, rudolph,\n",
      "Nearest to was: is, operatorname, coke, are, circ, UNK, be, to,\n",
      "Nearest to up: coke, deism, operatorname, UNK, ping, apocrypha, helmet, resort,\n",
      "Nearest to over: coke, quotas, zero, five, three, alzheimer, result, newsgroup,\n",
      "Nearest to years: ariane, emirates, weekend, coke, three, zero, agave, one,\n",
      "Nearest to four: eight, six, seven, zero, three, two, nine, five,\n",
      "Nearest to states: epistles, acacia, evangelicals, animated, ceiling, gb, and, coke,\n",
      "Nearest to nine: eight, six, one, zero, seven, two, five, three,\n",
      "Nearest to during: in, philippe, and, gland, five, operatorname, fellows, six,\n",
      "Nearest to other: UNK, gland, operatorname, associativity, ncaa, as, are, guitar,\n",
      "Nearest to be: is, was, are, have, operatorname, been, as, eight,\n",
      "Nearest to no: operatorname, that, one, gland, seven, a, rudolph, intent,\n",
      "Average loss at step  32000 :  5.010473691463471\n",
      "Average loss at step  34000 :  4.913040778398514\n",
      "Average loss at step  36000 :  4.9319108276367185\n",
      "Average loss at step  38000 :  5.041463543653488\n",
      "Average loss at step  40000 :  5.094651171207428\n",
      "Nearest to as: operatorname, UNK, for, and, by, albuquerque, gland, coke,\n",
      "Nearest to than: and, zero, operatorname, for, three, or, coke, gland,\n",
      "Nearest to they: he, to, operatorname, UNK, it, circ, acacia, that,\n",
      "Nearest to but: and, operatorname, gland, coke, UNK, in, eight, that,\n",
      "Nearest to some: the, emirates, operatorname, that, zero, and, two, UNK,\n",
      "Nearest to was: is, operatorname, are, were, be, UNK, coke, three,\n",
      "Nearest to up: deism, coke, operatorname, albuquerque, ping, resort, helmet, apocrypha,\n",
      "Nearest to over: zero, five, coke, and, two, quotas, four, eight,\n",
      "Nearest to years: ariane, two, three, emirates, zero, weekend, coke, albuquerque,\n",
      "Nearest to four: six, three, five, seven, eight, zero, two, nine,\n",
      "Nearest to states: epistles, acacia, UNK, operatorname, coke, and, evangelicals, gb,\n",
      "Nearest to nine: eight, six, seven, zero, four, one, five, three,\n",
      "Nearest to during: albuquerque, in, of, maynard, philippe, and, gland, fellows,\n",
      "Nearest to other: the, UNK, gland, are, operatorname, associativity, ncaa, albuquerque,\n",
      "Nearest to be: is, was, have, are, operatorname, been, UNK, he,\n",
      "Nearest to no: the, operatorname, a, seven, that, gland, it, one,\n",
      "Average loss at step  42000 :  4.992688352465629\n",
      "Average loss at step  44000 :  4.990653475880623\n",
      "Average loss at step  46000 :  4.9247091699242596\n",
      "Average loss at step  48000 :  4.933151039123535\n",
      "Average loss at step  50000 :  4.966469425797462\n",
      "Nearest to as: operatorname, gland, for, UNK, by, is, or, in,\n",
      "Nearest to than: and, or, for, two, operatorname, three, four, but,\n",
      "Nearest to they: he, to, it, operatorname, that, circ, acacia, bpp,\n",
      "Nearest to but: and, operatorname, gland, UNK, coke, in, it, that,\n",
      "Nearest to some: the, operatorname, many, UNK, emirates, a, that, and,\n",
      "Nearest to was: is, were, be, are, operatorname, had, he, acacia,\n",
      "Nearest to up: operatorname, coke, deism, albuquerque, five, ping, which, two,\n",
      "Nearest to over: coke, two, five, to, three, quotas, alzheimer, newsgroup,\n",
      "Nearest to years: ariane, two, three, weekend, emirates, albuquerque, coke, agave,\n",
      "Nearest to four: three, six, seven, five, eight, two, zero, nine,\n",
      "Nearest to states: epistles, acacia, nine, UNK, s, operatorname, coke, evangelicals,\n",
      "Nearest to nine: eight, seven, zero, four, six, five, three, two,\n",
      "Nearest to during: in, albuquerque, maynard, operatorname, gland, philippe, and, of,\n",
      "Nearest to other: are, the, operatorname, UNK, gland, associativity, albuquerque, loran,\n",
      "Nearest to be: was, have, is, are, been, operatorname, were, it,\n",
      "Nearest to no: operatorname, a, that, seven, four, it, gland, albuquerque,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  52000 :  4.898776931524277\n",
      "Average loss at step  54000 :  4.906833098888397\n",
      "Average loss at step  56000 :  4.925198348999023\n",
      "Average loss at step  58000 :  4.773573890924454\n",
      "Average loss at step  60000 :  4.86030411028862\n",
      "Nearest to as: operatorname, for, UNK, gland, or, circ, by, is,\n",
      "Nearest to than: or, for, but, and, three, four, operatorname, six,\n",
      "Nearest to they: he, it, to, operatorname, that, not, acacia, bpp,\n",
      "Nearest to but: operatorname, and, gland, coke, it, that, which, or,\n",
      "Nearest to some: many, the, operatorname, emirates, two, that, and, this,\n",
      "Nearest to was: is, were, be, operatorname, had, are, has, coke,\n",
      "Nearest to up: operatorname, coke, deism, which, four, eight, albuquerque, seven,\n",
      "Nearest to over: coke, five, two, three, to, four, zero, in,\n",
      "Nearest to years: three, ariane, two, one, weekend, nine, coke, emirates,\n",
      "Nearest to four: five, seven, three, six, eight, two, zero, nine,\n",
      "Nearest to states: s, UNK, epistles, in, acacia, and, operatorname, animated,\n",
      "Nearest to nine: eight, seven, six, four, one, zero, five, two,\n",
      "Nearest to during: in, albuquerque, of, maynard, for, nine, gland, operatorname,\n",
      "Nearest to other: are, operatorname, gland, their, many, associativity, albuquerque, loran,\n",
      "Nearest to be: have, was, is, are, been, operatorname, were, it,\n",
      "Nearest to no: operatorname, that, seven, four, which, albuquerque, intent, rudolph,\n",
      "Average loss at step  62000 :  4.922421825408936\n",
      "Average loss at step  64000 :  4.872574265241623\n",
      "Average loss at step  66000 :  4.853299182534218\n",
      "Average loss at step  68000 :  4.839088243961334\n",
      "Average loss at step  70000 :  4.87930759716034\n",
      "Nearest to as: operatorname, by, gland, in, UNK, coke, or, albuquerque,\n",
      "Nearest to than: for, but, or, four, two, operatorname, and, six,\n",
      "Nearest to they: he, it, not, operatorname, zero, be, to, acacia,\n",
      "Nearest to but: operatorname, and, gland, that, coke, it, which, zero,\n",
      "Nearest to some: many, zero, operatorname, the, all, that, emirates, two,\n",
      "Nearest to was: is, were, be, had, has, operatorname, been, are,\n",
      "Nearest to up: operatorname, coke, deism, which, albuquerque, five, eight, ping,\n",
      "Nearest to over: coke, five, from, zero, four, in, three, six,\n",
      "Nearest to years: one, three, ariane, two, weekend, coke, emirates, albuquerque,\n",
      "Nearest to four: six, seven, three, five, eight, two, nine, zero,\n",
      "Nearest to states: UNK, epistles, acacia, operatorname, in, coke, one, animated,\n",
      "Nearest to nine: eight, seven, four, six, zero, three, five, one,\n",
      "Nearest to during: in, albuquerque, nine, of, and, zero, gland, four,\n",
      "Nearest to other: are, operatorname, UNK, their, gland, many, zero, associativity,\n",
      "Nearest to be: have, was, been, is, are, were, operatorname, eight,\n",
      "Nearest to no: operatorname, that, a, four, albuquerque, seven, two, gland,\n",
      "Average loss at step  72000 :  4.8244444431066515\n",
      "Average loss at step  74000 :  4.855984924554825\n",
      "Average loss at step  76000 :  4.705298203706741\n",
      "Average loss at step  78000 :  4.58929161632061\n",
      "Average loss at step  80000 :  4.844659863948822\n",
      "Nearest to as: zero, operatorname, gland, albuquerque, and, coke, UNK, circ,\n",
      "Nearest to than: or, but, four, two, five, three, six, for,\n",
      "Nearest to they: he, it, to, not, operatorname, zero, acacia, that,\n",
      "Nearest to but: and, operatorname, that, zero, which, it, gland, coke,\n",
      "Nearest to some: many, the, operatorname, zero, all, other, three, various,\n",
      "Nearest to was: is, were, has, had, be, operatorname, zero, one,\n",
      "Nearest to up: operatorname, coke, five, eight, deism, seven, which, one,\n",
      "Nearest to over: coke, zero, three, five, four, from, seven, operatorname,\n",
      "Nearest to years: three, zero, one, ariane, two, four, weekend, coke,\n",
      "Nearest to four: three, five, six, seven, eight, zero, two, one,\n",
      "Nearest to states: epistles, zero, acacia, operatorname, animated, coke, in, gb,\n",
      "Nearest to nine: eight, zero, seven, six, four, five, three, one,\n",
      "Nearest to during: in, albuquerque, zero, of, and, four, gland, operatorname,\n",
      "Nearest to other: are, many, some, their, operatorname, zero, four, three,\n",
      "Nearest to be: have, was, been, is, were, are, twh, operatorname,\n",
      "Nearest to no: operatorname, that, zero, eight, a, four, not, seven,\n",
      "Average loss at step  82000 :  4.780763507246971\n",
      "Average loss at step  84000 :  4.709390733599663\n",
      "Average loss at step  86000 :  4.778694649100304\n",
      "Average loss at step  88000 :  4.752725266575814\n",
      "Average loss at step  90000 :  4.800429085969925\n",
      "Nearest to as: operatorname, gland, and, albuquerque, coke, by, for, UNK,\n",
      "Nearest to than: or, but, two, for, operatorname, zero, and, five,\n",
      "Nearest to they: he, it, not, operatorname, to, acacia, zero, these,\n",
      "Nearest to but: and, operatorname, which, it, gland, coke, that, or,\n",
      "Nearest to some: many, two, all, operatorname, that, other, most, the,\n",
      "Nearest to was: is, were, has, are, operatorname, had, be, he,\n",
      "Nearest to up: operatorname, coke, UNK, albuquerque, deism, seven, which, five,\n",
      "Nearest to over: coke, zero, three, four, from, five, in, to,\n",
      "Nearest to years: three, one, two, ariane, coke, albuquerque, weekend, zero,\n",
      "Nearest to four: five, six, three, seven, eight, two, zero, nine,\n",
      "Nearest to states: acacia, of, operatorname, epistles, coke, in, animated, seven,\n",
      "Nearest to nine: eight, seven, zero, six, five, four, operatorname, two,\n",
      "Nearest to during: in, albuquerque, nine, after, gland, first, as, of,\n",
      "Nearest to other: many, their, some, are, operatorname, UNK, these, gland,\n",
      "Nearest to be: have, been, was, is, are, were, twh, operatorname,\n",
      "Nearest to no: operatorname, a, that, two, albuquerque, seven, not, it,\n",
      "Average loss at step  92000 :  4.802169389486313\n",
      "Average loss at step  94000 :  4.774986459612847\n",
      "Average loss at step  96000 :  4.780175535798072\n",
      "Average loss at step  98000 :  4.693471085309982\n",
      "Average loss at step  100000 :  4.696833114981652\n",
      "Nearest to as: operatorname, gland, albuquerque, by, seven, coke, zero, for,\n",
      "Nearest to than: or, but, and, for, three, zero, five, operatorname,\n",
      "Nearest to they: he, it, not, operatorname, to, acacia, but, we,\n",
      "Nearest to but: and, operatorname, which, it, two, gland, coke, zero,\n",
      "Nearest to some: many, all, the, other, two, operatorname, UNK, this,\n",
      "Nearest to was: is, were, has, had, be, are, operatorname, eight,\n",
      "Nearest to up: operatorname, coke, seven, albuquerque, deism, UNK, eight, which,\n",
      "Nearest to over: five, three, coke, four, in, from, seven, operatorname,\n",
      "Nearest to years: three, UNK, zero, ariane, one, four, coke, five,\n",
      "Nearest to four: six, three, five, seven, two, eight, one, operatorname,\n",
      "Nearest to states: acacia, operatorname, epistles, animated, coke, nine, one, of,\n",
      "Nearest to nine: eight, seven, zero, six, one, operatorname, three, two,\n",
      "Nearest to during: in, albuquerque, after, of, as, by, six, gland,\n",
      "Nearest to other: some, many, are, these, their, operatorname, two, four,\n",
      "Nearest to be: have, is, was, been, are, were, seven, operatorname,\n",
      "Nearest to no: a, operatorname, that, three, five, six, the, seven,\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_steps = 100001 #100K steps\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # Open a writer to write summaries.\n",
    "    writer = tf.summary.FileWriter(FLAGS.log_dir, session.graph)\n",
    "\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips,skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        # Also, evaluate the merged op to get all summaries from the returned \"summary\" variable.\n",
    "        # Feed metadata variable to session for visualizing the graph in TensorBoard.\n",
    "        _, summary, loss_val = session.run(\n",
    "                [optimizer, merged, loss],\n",
    "                feed_dict=feed_dict,\n",
    "                run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        # Add returned summaries to writer in each step.\n",
    "        writer.add_summary(summary, step)\n",
    "        # Add metadata to visualize the graph for the last run.\n",
    "        if step == (num_steps - 1):\n",
    "            writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8    # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    # Write corresponding labels for the embeddings.\n",
    "    with open(FLAGS.log_dir + '/metadata.tsv', 'w') as f:\n",
    "        for i in xrange(vocabulary_size):\n",
    "            f.write(reverse_dictionary[i] + '\\n')\n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, os.path.join(FLAGS.log_dir, 'model.ckpt'))\n",
    "\n",
    "    # Create a configuration for visualizing embeddings with the labels in TensorBoard.\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embeddings.name\n",
    "    embedding_conf.metadata_path = os.path.join(FLAGS.log_dir, 'metadata.tsv')\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "    \n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 6: Visualize the embeddings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=missing-docstring\n",
    "# Function to draw visualization of distance between embeddings.\n",
    "def plot_with_labels(low_dim_embs, labels, filename):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(18, 18))    # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(\n",
    "                label,\n",
    "                xy=(x, y),\n",
    "                xytext=(5, 2),\n",
    "                textcoords='offset points',\n",
    "                ha='right',\n",
    "                va='bottom')\n",
    "\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "try:\n",
    "    # pylint: disable=g-import-not-at-top\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    tsne = TSNE(\n",
    "            perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "    plot_with_labels(low_dim_embs, labels, os.path.join(\"path to add\", 'tsne.png'))\n",
    "\n",
    "except ImportError as ex:\n",
    "    print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Rough__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Other ways of extracting Text8Corpus__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models.word2vec import Text8Corpus\n",
    "#corpus=Text8Corpus('text8')\n",
    "#corpus is a list of words similar to vocabulary that we produced above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text8():\n",
    "    words = open('text8').read()\n",
    "    word2idx = {}\n",
    "    sents = [[]]\n",
    "    count = 0\n",
    "    for word in words.split():\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = count\n",
    "            count += 1\n",
    "        sents[0].append(word2idx[word])\n",
    "    print(\"count:\", count)\n",
    "    return sents, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('text8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism\n",
      "originated\n",
      "as\n",
      "a\n",
      "term\n",
      "of\n",
      "abuse\n",
      "first\n",
      "used\n",
      "against\n",
      "early\n",
      "working\n",
      "class\n",
      "radicals\n",
      "including\n",
      "the\n",
      "diggers\n",
      "of\n",
      "the\n",
      "english\n",
      "revolution\n",
      "and\n",
      "the\n",
      "sans\n",
      "culottes\n"
     ]
    }
   ],
   "source": [
    "for word in words.split()[0:25]:\n",
    "    print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 253854\n"
     ]
    }
   ],
   "source": [
    "sentences, word2idx = get_text8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no. of sentences\n",
    "len(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253854"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
