{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import codecs\n",
    "from collections import Counter\n",
    "import collections\n",
    "import itertools\n",
    "from functools import partial\n",
    "import logging\n",
    "from math import log\n",
    "import os.path\n",
    "import pickle\n",
    "from random import shuffle\n",
    "\n",
    "import msgpack\n",
    "import numpy as np\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Text8Corpus\n",
    "corpus=Text8Corpus('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=list(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)\n",
    "#first 10,000 words of Text8 corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism', 'originated', 'as', 'a', 'term']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(corpus):\n",
    "    \"\"\"\n",
    "    Build a vocabulary with word frequencies for an entire corpus.\n",
    "    Returns a dictionary `w -> (i, f)`, mapping word strings to pairs of\n",
    "    word ID and word corpus frequency.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Building vocab from corpus\")\n",
    "\n",
    "    vocab = Counter(corpus)\n",
    "\n",
    "    logger.info(\"Done building vocab from corpus.\")\n",
    "\n",
    "    return {word: (i, freq) for i, (word, freq) in enumerate(vocab.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2520"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism (0, 102)\n",
      "originated (1, 2)\n",
      "as (2, 133)\n",
      "a (3, 184)\n",
      "term (4, 16)\n"
     ]
    }
   ],
   "source": [
    "for key in list(vocab)[:5]:\n",
    "    print(key, vocab[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 anarchism\n",
      "1 originated\n",
      "2 as\n",
      "3 a\n",
      "4 term\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(corpus[0:5]):\n",
    "    print (i,line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccur(vocab, corpus, window_size=10, min_count=None):\n",
    "    \"\"\"\n",
    "    Build a word co-occurrence list for the given corpus.\n",
    "    This function is a tuple generator, where each element (representing\n",
    "    a cooccurrence pair) is of the form\n",
    "        (i_main, i_context, cooccurrence)\n",
    "    where `i_main` is the ID of the main word in the cooccurrence and\n",
    "    `i_context` is the ID of the context word, and `cooccurrence` is the\n",
    "    `X_{ij}` cooccurrence value as described in Pennington et al.\n",
    "    (2014).\n",
    "    If `min_count` is not `None`, cooccurrence pairs where either word\n",
    "    occurs in the corpus fewer than `min_count` times are ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    id2word = dict((i, word) for word, (i, _) in vocab.items())\n",
    "\n",
    "    # Collect cooccurrences internally as a sparse matrix for passable\n",
    "    # indexing speed; we'll convert into a list later\n",
    "    cooccurrences = sparse.lil_matrix((vocab_size, vocab_size),\n",
    "                                      dtype=np.float64)\n",
    "    tokens = corpus\n",
    "    token_ids = [vocab[word][0] for word in tokens]\n",
    "\n",
    "    for center_i, center_id in enumerate(token_ids):\n",
    "        # Collect all word IDs in left window of center word\n",
    "        context_ids = token_ids[max(0, center_i - window_size) : center_i]\n",
    "        contexts_len = len(context_ids)\n",
    "\n",
    "        for left_i, left_id in enumerate(context_ids):\n",
    "            # Distance from center word\n",
    "            distance = contexts_len - left_i\n",
    "\n",
    "            # Weight by inverse of distance between words\n",
    "            increment = 1.0 / float(distance)\n",
    "\n",
    "            # Build co-occurrence matrix symmetrically (pretend we\n",
    "            # are calculating right contexts as well)\n",
    "            cooccurrences[center_id, left_id] += increment\n",
    "            cooccurrences[left_id, center_id] += increment\n",
    "\n",
    "    # Now yield our tuple sequence (dig into the LiL-matrix internals to\n",
    "    # quickly iterate through all nonzero cells)\n",
    "    for i, (row, data) in enumerate(zip(cooccurrences.rows,\n",
    "                                                   cooccurrences.data)):\n",
    "        if min_count is not None and vocab[id2word[i]][1] < min_count:\n",
    "            continue\n",
    "\n",
    "        for data_idx, j in enumerate(row):\n",
    "            if min_count is not None and vocab[id2word[j]][1] < min_count:\n",
    "                continue\n",
    "\n",
    "            yield i, j, data[data_idx]\n",
    "    #return cooccurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrences=build_cooccur(vocab, corpus, window_size=10, min_count=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iter(vocab, data, learning_rate=0.05, x_max=100, alpha=0.75):\n",
    "    \"\"\"\n",
    "    Run a single iteration of GloVe training using the given\n",
    "    cooccurrence data and the previously computed weight vectors /\n",
    "    biases and accompanying gradient histories.\n",
    "    `data` is a pre-fetched data / weights list where each element is of\n",
    "    the form\n",
    "        (v_main, v_context,\n",
    "         b_main, b_context,\n",
    "         gradsq_W_main, gradsq_W_context,\n",
    "         gradsq_b_main, gradsq_b_context,\n",
    "         cooccurrence)\n",
    "    as produced by the `train_glove` function. Each element in this\n",
    "    tuple is an `ndarray` view into the data structure which contains\n",
    "    it.\n",
    "    See the `train_glove` function for information on the shapes of `W`,\n",
    "    `biases`, `gradient_squared`, `gradient_squared_biases` and how they\n",
    "    should be initialized.\n",
    "    The parameters `x_max`, `alpha` define our weighting function when\n",
    "    computing the cost for two word pairs; see the GloVe paper for more\n",
    "    details.\n",
    "    Returns the cost associated with the given weight assignments and\n",
    "    updates the weights by online AdaGrad in place.\n",
    "    \"\"\"\n",
    "\n",
    "    global_cost = 0\n",
    "\n",
    "    # We want to iterate over data randomly so as not to unintentionally\n",
    "    # bias the word vector contents\n",
    "    shuffle(data)\n",
    "\n",
    "    for (v_main, v_context, b_main, b_context, gradsq_W_main, gradsq_W_context,\n",
    "         gradsq_b_main, gradsq_b_context, cooccurrence) in data:\n",
    "\n",
    "        weight = (cooccurrence / x_max) ** alpha if cooccurrence < x_max else 1\n",
    "\n",
    "        # Compute inner component of cost function, which is used in\n",
    "        # both overall cost calculation and in gradient calculation\n",
    "        #\n",
    "        #   $$ J' = w_i^Tw_j + b_i + b_j - log(X_{ij}) $$\n",
    "        cost_inner = (v_main.dot(v_context)\n",
    "                      + b_main[0] + b_context[0]\n",
    "                      - log(cooccurrence))\n",
    "\n",
    "        # Compute cost\n",
    "        #\n",
    "        #   $$ J = f(X_{ij}) (J')^2 $$\n",
    "        cost = weight * (cost_inner ** 2)\n",
    "\n",
    "        # Add weighted cost to the global cost tracker\n",
    "        global_cost += 0.5 * cost\n",
    "\n",
    "        # Compute gradients for word vector terms.\n",
    "        #\n",
    "        # NB: `main_word` is only a view into `W` (not a copy), so our\n",
    "        # modifications here will affect the global weight matrix;\n",
    "        # likewise for context_word, biases, etc.\n",
    "        grad_main = weight * cost_inner * v_context\n",
    "        grad_context = weight * cost_inner * v_main\n",
    "\n",
    "        # Compute gradients for bias terms\n",
    "        grad_bias_main = weight * cost_inner\n",
    "        grad_bias_context = weight * cost_inner\n",
    "\n",
    "        # Now perform adaptive updates\n",
    "        v_main -= (learning_rate * grad_main / np.sqrt(gradsq_W_main))\n",
    "        v_context -= (learning_rate * grad_context / np.sqrt(gradsq_W_context))\n",
    "\n",
    "        b_main -= (learning_rate * grad_bias_main / np.sqrt(gradsq_b_main))\n",
    "        b_context -= (learning_rate * grad_bias_context / np.sqrt(\n",
    "                gradsq_b_context))\n",
    "\n",
    "        # Update squared gradient sums\n",
    "        gradsq_W_main += np.square(grad_main)\n",
    "        gradsq_W_context += np.square(grad_context)\n",
    "        gradsq_b_main += grad_bias_main ** 2\n",
    "        gradsq_b_context += grad_bias_context ** 2\n",
    "\n",
    "    return global_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_glove(vocab, cooccurrences, iter_callback=None, vector_size=100,\n",
    "                iterations=25):\n",
    "    \"\"\"\n",
    "    Train GloVe vectors on the given generator `cooccurrences`, where\n",
    "    each element is of the form\n",
    "        (word_i_id, word_j_id, x_ij)\n",
    "    where `x_ij` is a cooccurrence value $X_{ij}$ as presented in the\n",
    "    matrix defined by `build_cooccur` and the Pennington et al. (2014)\n",
    "    paper itself.\n",
    "    If `iter_callback` is not `None`, the provided function will be\n",
    "    called after each iteration with the learned `W` matrix so far.\n",
    "    Keyword arguments are passed on to the iteration step function\n",
    "    `run_iter`.\n",
    "    Returns the computed word vector matrix `W`.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Word vector matrix. This matrix is (2V) * d, where N is the size\n",
    "    # of the corpus vocabulary and d is the dimensionality of the word\n",
    "    # vectors. All elements are initialized randomly in the range (-0.5,\n",
    "    # 0.5]. We build two word vectors for each word: one for the word as\n",
    "    # the main (center) word and one for the word as a context word.\n",
    "    #\n",
    "    # It is up to the client to decide what to do with the resulting two\n",
    "    # vectors. Pennington et al. (2014) suggest adding or averaging the\n",
    "    # two for each word, or discarding the context vectors.\n",
    "    W = (np.random.rand(vocab_size * 2, vector_size) - 0.5) / float(vector_size + 1)\n",
    "\n",
    "    # Bias terms, each associated with a single vector. An array of size\n",
    "    # $2V$, initialized randomly in the range (-0.5, 0.5].\n",
    "    biases = (np.random.rand(vocab_size * 2) - 0.5) / float(vector_size + 1)\n",
    "\n",
    "    # Training is done via adaptive gradient descent (AdaGrad). To make\n",
    "    # this work we need to store the sum of squares of all previous\n",
    "    # gradients.\n",
    "    #\n",
    "    # Like `W`, this matrix is (2V) * d.\n",
    "    #\n",
    "    # Initialize all squared gradient sums to 1 so that our initial\n",
    "    # adaptive learning rate is simply the global learning rate.\n",
    "    gradient_squared = np.ones((vocab_size * 2, vector_size),\n",
    "                               dtype=np.float64)\n",
    "\n",
    "    # Sum of squared gradients for the bias terms.\n",
    "    gradient_squared_biases = np.ones(vocab_size * 2, dtype=np.float64)\n",
    "\n",
    "    # Build a reusable list from the given cooccurrence generator,\n",
    "    # pre-fetching all necessary data.\n",
    "    #\n",
    "    # NB: These are all views into the actual data matrices, so updates\n",
    "    # to them will pass on to the real data structures\n",
    "    #\n",
    "    # (We even extract the single-element biases as slices so that we\n",
    "    # can use them as views)\n",
    "    data = [(W[i_main], W[i_context + vocab_size],\n",
    "             biases[i_main : i_main + 1],\n",
    "             biases[i_context + vocab_size : i_context + vocab_size + 1],\n",
    "             gradient_squared[i_main], gradient_squared[i_context + vocab_size],\n",
    "             gradient_squared_biases[i_main : i_main + 1],\n",
    "             gradient_squared_biases[i_context + vocab_size\n",
    "                                     : i_context + vocab_size + 1],\n",
    "             cooccurrence)\n",
    "            for i_main, i_context, cooccurrence in cooccurrences]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        print (\"\\tBeginning iteration %i..\", i)\n",
    "\n",
    "        cost = run_iter(vocab, data)\n",
    "\n",
    "        print (\"\\t\\tDone (cost %f)\", cost)\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBeginning iteration %i.. 0\n",
      "\t\tDone (cost %f) 1633.813681868638\n",
      "\tBeginning iteration %i.. 1\n",
      "\t\tDone (cost %f) 1546.7335233659032\n",
      "\tBeginning iteration %i.. 2\n",
      "\t\tDone (cost %f) 1492.2095841455784\n",
      "\tBeginning iteration %i.. 3\n",
      "\t\tDone (cost %f) 1448.4029836101452\n",
      "\tBeginning iteration %i.. 4\n",
      "\t\tDone (cost %f) 1410.0669242591334\n",
      "\tBeginning iteration %i.. 5\n",
      "\t\tDone (cost %f) 1371.893303342185\n",
      "\tBeginning iteration %i.. 6\n",
      "\t\tDone (cost %f) 1324.6154488334184\n",
      "\tBeginning iteration %i.. 7\n",
      "\t\tDone (cost %f) 1257.77416891155\n",
      "\tBeginning iteration %i.. 8\n",
      "\t\tDone (cost %f) 1177.8323698698223\n",
      "\tBeginning iteration %i.. 9\n",
      "\t\tDone (cost %f) 1107.1972404600506\n",
      "\tBeginning iteration %i.. 10\n",
      "\t\tDone (cost %f) 1052.4688812422248\n",
      "\tBeginning iteration %i.. 11\n",
      "\t\tDone (cost %f) 1008.3859598927886\n",
      "\tBeginning iteration %i.. 12\n",
      "\t\tDone (cost %f) 971.4789869811711\n",
      "\tBeginning iteration %i.. 13\n",
      "\t\tDone (cost %f) 940.10302386145\n",
      "\tBeginning iteration %i.. 14\n",
      "\t\tDone (cost %f) 913.1234174952546\n",
      "\tBeginning iteration %i.. 15\n",
      "\t\tDone (cost %f) 889.7304496991021\n",
      "\tBeginning iteration %i.. 16\n",
      "\t\tDone (cost %f) 869.1506313203968\n",
      "\tBeginning iteration %i.. 17\n",
      "\t\tDone (cost %f) 851.1225892459131\n",
      "\tBeginning iteration %i.. 18\n",
      "\t\tDone (cost %f) 834.9471338488174\n",
      "\tBeginning iteration %i.. 19\n",
      "\t\tDone (cost %f) 820.4636025111064\n",
      "\tBeginning iteration %i.. 20\n",
      "\t\tDone (cost %f) 807.3899559116768\n",
      "\tBeginning iteration %i.. 21\n",
      "\t\tDone (cost %f) 795.4911092595314\n",
      "\tBeginning iteration %i.. 22\n",
      "\t\tDone (cost %f) 784.5304220943998\n",
      "\tBeginning iteration %i.. 23\n",
      "\t\tDone (cost %f) 774.464367701125\n",
      "\tBeginning iteration %i.. 24\n",
      "\t\tDone (cost %f) 765.0647777484472\n"
     ]
    }
   ],
   "source": [
    "W=train_glove(vocab, cooccurrences, iter_callback=None, vector_size=100,\n",
    "                iterations=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5040, 100)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((id, word) for word, (id, _) in vocab.items())\n",
    "word2id = dict((word, id) for word, (id, _) in vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize word vectors\n",
    "for i, row in enumerate(W):\n",
    "    W[i, :] /= np.linalg.norm(row)\n",
    "    \n",
    "# Remove context word vectors\n",
    "W = W[:len(vocab), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2520, 100)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(positive, negative=[], topn=5, freq_threshold=2):\n",
    "    # Build a \"mean\" vector for the given positive and negative terms\n",
    "    mean_vecs = []\n",
    "    for word in positive: mean_vecs.append(W[vocab[word][0]])\n",
    "    if negative!=[]:\n",
    "        for word in negative: mean_vecs.append(-1 * W[vocab[word][0]])\n",
    "    \n",
    "    mean = np.array(mean_vecs).mean(axis=0)\n",
    "    mean /= np.linalg.norm(mean)\n",
    "    \n",
    "    # Now calculate cosine distances between this mean vector and all others\n",
    "    dists = np.dot(W, mean)\n",
    "    \n",
    "    best = np.argsort(dists)[::-1][:topn + len(positive) + len(negative) + 100]\n",
    "    result = [(id2word[i], dists[i]) for i in best if (vocab[id2word[i]][1] > freq_threshold\n",
    "                                                       and id2word[i] not in positive\n",
    "                                                       and id2word[i] not in negative)]\n",
    "    return result[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('belief', 0.6104455478001918),\n",
       " ('development', 0.5917980140334254),\n",
       " ('life', 0.5897436495730175),\n",
       " ('labour', 0.588090635366689),\n",
       " ('thought', 0.5870572364315194)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(['history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [\n",
    "#           'Text of the first document',\n",
    "#           'Text of the second document made longer',\n",
    "#           'Number three',\n",
    "#           'This is number four',\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_vocab(corpus):\n",
    "#     \"\"\"\n",
    "#     Build a vocabulary with word frequencies for an entire corpus.\n",
    "#     Returns a dictionary `w -> (i, f)`, mapping word strings to pairs of\n",
    "#     word ID and word corpus frequency.\n",
    "#     \"\"\"\n",
    "\n",
    "#     logger.info(\"Building vocab from corpus\")\n",
    "\n",
    "#     vocab = Counter()\n",
    "#     for line in corpus:\n",
    "#         tokens = line.strip().split()\n",
    "#         vocab.update(tokens)\n",
    "\n",
    "#     logger.info(\"Done building vocab from corpus.\")\n",
    "\n",
    "#     return {word: (i, freq) for i, (word, freq) in enumerate(vocab.items())}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
